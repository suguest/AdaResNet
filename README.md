# AdaResNet: Enhancing ResNet with Adaptive Weighting of Input Data and Processed Data

In very deep networks, gradients can become very small during backpropagation, making it difficult to train the early layers. ResNet (Residual Network) addresses this by allowing gradients to flow directly through the network via skip connections, facilitating the training of much deeper networks. 

However, in the process of skip connections, the input data (ipd) is directly added to the transformed data (tfd), treating ipd and tfd as the same, instead of adapting to different scenarios. 

In this project, we propose AdaResNet (Auto-Adapting Residual Network), which automatically adapts the ratio of ipd and tfd with respect to the training data. We introduce a variable, weight, to represent this ratio. This variable is adjusted automatically during the backpropagation process, allowing it to adapt to the training data rather than remaining fixed. The verification results show that AdaResNet achieves higher accuracy and faster training speeds.

For any advice to improve this project or questions about this porject, please contact suguest@126.com or suhong@cuit.edu.cn.

Verification:
  We use the cifar10 test data to do the verificaion.
  
Method. We compare the ResNet50 and AdaResNet based on Resnet50 to comparison, named cifar10Test_ResNet50.py and cifar10Test_AdaResNet.py separately.
    The main change in cifar10Test_AdaResNet.py is as follows:„ÄÅ
      class AddWithWeight(layers.Layer):
        def __init__(self, **kwargs):
            super(AddWithWeight, self).__init__(**kwargs)
    
        def build(self, input_shape):
            self.Weight = self.add_weight(name='Weight', 
                                        shape=(1,),
                                        initializer='zeros',
                                        trainable=True)
            super(AddWithWeight, self).build(input_shape)
    
        def call(self, inputs):
            x, d = inputs
            return d + self.Weight * x



    def identity_block(x, filters, kernel_size=3):
        x_skip = x
        
        x = layers.Conv2D(filters, kernel_size, padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        
        x = layers.Conv2D(filters, kernel_size, padding='same')(x)
        x = layers.BatchNormalization()(x)
        
        x = AddWithWeight()([x_skip, x])
        x = layers.ReLU()(x)
        
        return x
    
    def convolutional_block(x, filters, kernel_size=3, strides=2):
        x_skip = x
        
        x = layers.Conv2D(filters, kernel_size, strides=strides, padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        
        x = layers.Conv2D(filters, kernel_size, padding='same')(x)
        x = layers.BatchNormalization()(x)
        
        x_skip = layers.Conv2D(filters, 1, strides=strides, padding='same')(x_skip)
        x_skip = layers.BatchNormalization()(x_skip)
        
        x = AddWithWeight()([x_skip, x])
        x = layers.ReLU()(x)
        
        return x
    
    # 
    def build_custom_resnet(input_shape, num_classes):
        inputs = layers.Input(shape=input_shape)
        
        x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)
        
        x = convolutional_block(x, 64)
        x = identity_block(x, 64)
        
        x = convolutional_block(x, 128)
        x = identity_block(x, 128)
        
        x = convolutional_block(x, 256)
        x = identity_block(x, 256)
        
        x = convolutional_block(x, 512)
        x = identity_block(x, 512)
        
        x = layers.GlobalAveragePooling2D()(x)
        outputs = layers.Dense(num_classes, activation='softmax')(x)
        
        return models.Model(inputs, outputs)
 
